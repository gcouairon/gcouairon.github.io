---
title: Selected Publications
icon: fa-book
order: 4
---

<style>
.thumb {width:100%;border-radius:1em;display:block;}
table td {padding:1em;vertical-align:top;}
em {font-weight: 30;}
td:first-child {
            width: 30%;
        }
        td a {color:white;}
  </style>
See [Google Scholar]() for a complete list of my publications.


<div style="text-align: left; font-size:20px">
<table>

<tr>
<td><img src="assets/images/chm_overview.jpg" class="thumb" /></td>
<td>
<b>Sub‐meter resolution canopy height maps using self‐supervised learning and a vision transformer trained on Aerial and GEDI Lidar, Arxiv 2023</b>
<br> <i>J. Tolan, H. Yang, B. Nosarzewski, <u>G. Couairon</u>, H. Vo, John Brandt, J. Spore, S. Majumdar, D. Haziza, J. Vamaraju, T. Moutakanni, P. Bojanowski, T. Johns, B. White, T. Tiecke, C. Couprie </i>
<br> 
<button> <a href="https://arxiv.org/abs/2304.07213"> Paper</a></button>
<button> <a href='https://wri-datalab.earthengine.app/view/submeter-canopyheight'> Maps </a> </button>
<button> <a href='https://research.facebook.com/blog/2023/4/every-tree-counts-large-scale-mapping-of-canopy-height-at-the-resolution-of-individual-trees/'> Meta Blog Post </a></button>
</td>
</tr>

<tr>
<td><img src="assets/images/rsoups_overview.png" class="thumb" /></td>
<td>
<b>Rewarded soups: towards Pareto‐optimality by interpolating weights fine‐tuned on diverse rewards, NeurIPS 2023</b>
<br> <i> Alexandre Ramé, <u>Guillaume Couairon</u>, Corentin Dancette, Mustafa Shukor, J-B Gaya, Laure Soulier, Matthieu Cord </i>
<br> 
<button><a href="https://arxiv.org/abs/2306.04488"> Paper</a></button>
<button> <a href="https://github.com/alexrame/rewardedsoups"> Code </a></button>
<button> <a href="https://huggingface.co/spaces/alexrame/rewardedsoups"> Demo </a></button>

</td>
</tr>

<tr>
<td><img src="assets/images/zestguide_overview.jpg" class="thumb" /></td>
<td>
<b>Zero‐shot spatial layout conditioning for text‐to‐image diffusion models, ICCV 2023</b>
<br> <i> <u>Guillaume Couairon</u>, Marlène Careil, Matthieu Cord, Stéphane Lathuilière, Jakob Verbeek </i>
<br> 
<button> <a href="https://arxiv.org/abs/2306.13754"> Paper </a></button>
<button> <a href="">Code (soon) </a></button>
</td>
</tr>

<tr>
<td><img src="assets/images/stable_signature_overview.jpg" class="thumb" /></td>
<td>
<b>The stable signature: Rooting watermarks in latent diffusion models, ICCV 2023</b>
<br> <i>Pierre Fernandez, <u>Guillaume Couairon</u>, Hervé Jégou, Matthijs Douze, Teddy Furon </i>
<br> 
<button> <a href='https://arxiv.org/abs/2303.15435'> Paper </a></button>
<button> <a href='https://github.com/facebookresearch/stable_signature'> Code </a> </button>
<button> <a href='https://pierrefdz.github.io/publications/stablesignature/'> Blog Post </a></button>
<button> <a href='https://ai.meta.com/blog/stable-signature-watermarking-generative-ai/'> Meta Blog Post </a></button>
</td>
</tr>

<tr>
<td><img src="assets/images/diffedit/overview_half.png" class="thumb" /></td>
<td>
<b>DiffEdit: Diffusion‐based semantic image editing with mask guidance, ICLR 2023 Spotlight</b>
<br> <i> <u>Guillaume Couairon</u>, Jakob Verbeek, Holger Schwenk, Matthieu Cord </i>
<br> 
<button><a href="https://arxiv.org/abs/2210.11427"> Paper</a></button>
<button><a href="https://huggingface.co/docs/diffusers/using-diffusers/diffedit"> HuggingFace Code </a></button>
<button><a href="2022/11/30/diffedit.html"> Blog Post</a></button>
</td>
</tr>

<tr>
<td><img src="assets/images/flexit_overview.jpg" class="thumb" /></td>
<td>
<b>FlexIT: Towards Flexible Semantic Image Translation, CVPR 2022</b>
<br> <i> <u>Guillaume Couairon</u>, Asya Grechka, Jakob Verbeek, Holger Schwenk, Matthieu Cord </i>
<br> 
<button> <a href="https://arxiv.org/abs/2203.04705">Paper</a></button>
<button> <a href="https://github.com/facebookresearch/SemanticImageTranslation">Code </a></button>
</td>
</tr>

<tr>
<td><img src="assets/images/flava_overview.jpg" class="thumb" /></td>
<td>
<b>FLAVA: A foundational language and vision alignment model, CVPR 2022</b>
<br> <i> Amanpreet Singh, Ronghang Hu, Vedanuj Goswami, <u>Guillaume Couairon</u>, Wojciech Galuba, Marcus Rohrbach, Douwe Kiela </i>
<br> 
<button><a href="https://arxiv.org/abs/2112.04482/"> Paper</a></button>
<button><a href="https://flava-model.github.io/"> Blog Post</a></button>
<button><a href="https://github.com/facebookresearch/multimodal/tree/main/examples/flava"> Code </a></button>
<button><a href="https://huggingface.co/docs/transformers/model_doc/flava"> Hugging Face Model </a></button>
</td>
</tr>

<tr>
<td><img src="assets/images/vicha_overview.jpg" class="thumb" /></td>
<td>
<b>Efficient vision‐language pretraining with visual concepts and hierarchical alignment, BMVC 2022</b>
<br> <i> Mustafa Schukor, <u>Guillaume Couairon</u>, Matthieu Cord </i>
<br> 
<button><a href="https://arxiv.org/abs/2208.13628"> Paper</a></button>
<button><a href="https://github.com/mshukor/ViCHA"> Code </a></button>
</td>
</tr>

</table>
